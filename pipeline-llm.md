[CI/CD preprocessing pipelines in LLM applications](https://circleci.com/blog/ci-cd-preprocessing-pipelines-in-llm-applications/)


[OpenManus](https://github.com/mannaandpoem/OpenManus)

Open-source framework for building and running AI agents, connects to tools and APIs, no invite code needed


[AutoCodeAgent2.0](https://github.com/samugit83/AutoCodeAgent2.0/tree/master)

[Build Real-Time RAG Pipelines with CocoIndex, Amazon S3, and AWS SQS](https://app.daily.dev/posts/build-real-time-rag-pipelines-with-cocoindex-amazon-s3-and-aws-sqs-znho0qjls)

[Reranking-RAG-pipelines](https://atalupadhyay.wordpress.com/2025/06/19/reranking-in-rag-pipelines-a-complete-guide-with-hands-on-implementation/)

[Building a High-Performance Parallel LLM Pipeline Using Weight Optimization, KV Cache, SDPA, and Kubernetes](https://levelup.gitconnected.com/building-a-high-performance-parallel-llm-pipeline-using-weight-optimization-kv-cache-sdpa-and-d02225f2b1d1)

[Serving LLM for 100K Parallel Queries](https://github.com/FareedKhan-dev/llm-scale-deploy-guide?source=post_page-----d02225f2b1d1---------------------------------------)

A Developer's Guide to Hosting 3B LLM for Millions of Queries in Parallel Whether you are building agents, RAG bots, or LLM apps, the core of your product is usually an LLM accessed via API. Providers like Together.ai optimize these models for efficient and scalable use, rather than hosting them in full precision.

