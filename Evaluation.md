[Self-improving evaluation in LangSmith](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/)

[Custom-Metrics](https://github.com/rajib76/ragas_examples/blob/main/custom_metrics/criminality.py)

[Custom LLM Evaluations ⚙️: Function Calling Agent](https://www.youtube.com/watch?v=EfhylWtNb1s)

[SummHay benchmark](https://github.com/salesforce/summary-of-a-haystack)

[Deepval-LlamaIndex](https://docs.confident-ai.com/docs/integrations-llamaindex) 

[DocBench](https://github.com/Anni-Zou/DocBench) 

[ragas_examples](https://github.com/rajib76/ragas_examples/blob/main/05_answer_correctness.py)

[langsmith-evaluation-helper](https://github.com/gaudiy/langsmith-evaluation-helper)


[LLM Hallucination Index](https://github.com/rungalileo/hallucination-index) 



[Running SWE-bench with LangSmith](https://docs.smith.langchain.com/tutorials/Developers/swe-benchmark)
