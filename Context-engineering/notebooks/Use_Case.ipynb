{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Meeting Analysis Use Case\n",
        "\n",
        "Copyright 2025-2026, Denis Rothman\n",
        "\n",
        "The notebook's goal is to demonstrate how to guide an AI through a multi-step analytical process, moving from a raw transcript to actionable insights, thereby training both the user and the AI.\n",
        "\n",
        "This notebook is a step-by-step guide on how you can engineer a structured context . This guide will serve as the architectural blueprint for your code, explaining not just the *what* but the *why* at each stage.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Denis2054/Context-Engineering-for-Multi-Agent-Systems/main/Chapter01/Use_Case_Flowhart.png\" alt=\"Flowchart\" width=\"800\"/>\n"
      ],
      "metadata": {
        "id": "FDBDBUKfSggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Setup and Preliminaries\n",
        "\n",
        "This section handles the basic configuration.\n",
        "\n",
        "1.  **Cell 1: Install Libraries**\n",
        "\n",
        "      * Install the necessary OpenAI library."
      ],
      "metadata": {
        "id": "ivJQbp_mwtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Installation\n",
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.14.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==2.14.0\n",
            "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.14.0) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.14.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.14.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.14.0) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (0.4.2)\n",
            "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "Successfully installed openai-2.14.0\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "voHBQKGtSggF",
        "outputId": "043f1ef0-8cdc-4746-e5cf-f66f5ea62676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 2: Imports and API Key**\n",
        "\n",
        "      * Import the library and securely prompt for the user's API key. This is better than hard-coding it."
      ],
      "metadata": {
        "id": "jtGvDLpLSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "eNHm5qkhSggG",
        "outputId": "f2e91a7b-7f98-40b9-a2e7-49ea7831898a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Cell 3: The Raw Data (The \"Crime Scene\")**\n",
        "\n",
        "      * Define the meeting transcript as a multi-line string. This is our primary data source."
      ],
      "metadata": {
        "id": "Vgp62z7OSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: The Full Meeting Transcript\n",
        "meeting_transcript = \"\"\"\n",
        "        Tom: Morning all. Coffee is still kicking in.\n",
        "        Sarah: Morning, Tom. Right, let's jump in. Project Phoenix timeline. Tom, you said the backend components are on track?\n",
        "        Tom: Mostly. We hit a small snag with the payment gateway integration. It's... more complex than the docs suggested. We might need another three days.\n",
        "        Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.\n",
        "        Sarah: I agree with Maria. What's the alternative, Tom?\n",
        "        Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.\n",
        "        Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?\n",
        "        Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.\n",
        "        Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?\n",
        "        Tom: Seems standard.\n",
        "        Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.\n",
        "        Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n",
        "        Tom: Sounds good. Now, more coffee.\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UJpZUVXpSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 1: Establishing the scope (The 'What')\n",
        "\n",
        "Here, we define the scope of the analysis. Each step's output will inform the next, creating a chain of context.\n",
        "\n",
        "1.  **Cell 4: g2 - Set the Signal-to-Noise Ratio**\n",
        "\n",
        "      * We'll start by cleaning the data. The prompt explicitly tells the AI to separate substantive content from conversational noise."
      ],
      "metadata": {
        "id": "40PTjmLaSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: g2 - Isolating Content from Noise\n",
        "prompt_g2 = f\"\"\"\n",
        "        Analyze the following meeting transcript. Your task is to isolate the substantive content from the conversational noise.\n",
        "        - Substantive content includes: decisions made, project updates, problems raised, and strategic suggestions.\n",
        "        - Noise includes: greetings, pleasantries, and off-topic remarks (like coffee).\n",
        "        Return ONLY the substantive content.\n",
        "\n",
        "        Transcript:\n",
        "        ---\n",
        "        {meeting_transcript}\n",
        "        ---\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "069eAXt9SggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    client = OpenAI()\n",
        "\n",
        "    response_g2 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_g2}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    substantive_content = response_g2.choices[0].message.content\n",
        "    print(\"--- SUBSTANTIVE CONTENT ---\")\n",
        "    print(substantive_content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ptfVZHlps3r4",
        "outputId": "c5f5216a-355e-4506-cd8c-d0a181fc770b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SUBSTANTIVE CONTENT ---\n",
            "- Project Phoenix backend is mostly on track, but payment gateway integration is more complex than expected and would add ~3 days.\n",
            "- Concern raised: a 3-day slip would compress final testing against the launch deadline due to lack of buffer.\n",
            "- Tentative decision: Tom will work over the weekend to recover the schedule and avoid delaying the timeline.\n",
            "- Front-end status: ahead of schedule with extra bandwidth available.\n",
            "- Marketing/launch strategy issue raised: a large social media push on launch day could overload servers if initial bugs exist.\n",
            "- Decision: do a soft launch (invite-only) for the first week, then follow with the big social media push.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 5: g3 - Define the Scope of Time (Simulated RAG)**\n",
        "\n",
        "      * We'll simulate a RAG context by providing a \"previous\" summary and asking for what's new. This teaches the user the importance of historical context."
      ],
      "metadata": {
        "id": "XY2ZlN8sSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: g3 - Identifying NEW Information (Simulated RAG)\n",
        "previous_summary = \"In our last meeting, we finalized the goals for Project Phoenix and assigned backend work to Tom and front-end to Maria.\"\n",
        "\n",
        "prompt_g3 = f\"\"\"\n",
        "Context: The summary of our last meeting was: \"{previous_summary}\"\n",
        "\n",
        "Task: Analyze the following substantive content from our new meeting. Identify and summarize ONLY the new developments, problems, or decisions that have occurred since the last meeting.\n",
        "\n",
        "New Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your chat completion request\n",
        "try:\n",
        "    response_g3 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g3}]\n",
        "    )\n",
        "    new_developments = response_g3.choices[0].message.content\n",
        "    print(\"--- NEW DEVELOPMENTS SINCE LAST MEETING ---\")\n",
        "    print(new_developments)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW DEVELOPMENTS SINCE LAST MEETING ---\n",
            "- **Backend risk identified:** Payment gateway integration is more complex than expected, potentially adding **~3 days** to the backend schedule (though backend is otherwise mostly on track).\n",
            "- **Schedule/testing concern raised:** A **3-day slip** would **compress final testing** and threaten the launch timeline due to limited buffer.\n",
            "- **Tentative mitigation decision:** **Tom will work over the weekend** to try to recover the schedule and avoid delaying the timeline.\n",
            "- **Front-end update:** Front-end work is **ahead of schedule**, creating **extra bandwidth** availability.\n",
            "- **Launch strategy problem identified:** A large launch-day social media push could **overload servers** if early bugs exist.\n",
            "- **Launch strategy decision:** Proceed with a **soft launch (invite-only) for the first week**, then do the **major social media push** afterward.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "1LhMAk4-SggH",
        "outputId": "67c3b862-a659-4f9e-e11a-c7ac1234d2f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 2: Conducting the Investigation (The 'How')\n",
        "\n",
        "Now we move from identifying facts to generating insights, the core of the semantic context interpreation journey.\n",
        "\n",
        "1.  **Cell 6: g4 - Identify the Key Threads**\n",
        "\n",
        "      * This is a crucial step. The prompt asks the AI to read between the lines."
      ],
      "metadata": {
        "id": "wh2UWr4ySggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: g4 - Uncovering Implicit Threads\n",
        "prompt_g4 = f\"\"\"\n",
        "Task: Analyze the following meeting content for implicit social dynamics and unstated feelings. Go beyond the literal words.\n",
        "- Did anyone seem hesitant or reluctant despite agreeing to something?\n",
        "- Were there any underlying disagreements or tensions?\n",
        "- What was the overall mood?\n",
        "\n",
        "Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g4 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g4}]\n",
        "    )\n",
        "    implicit_threads = response_g4.choices[0].message.content\n",
        "    print(\"--- IMPLICIT THREADS AND DYNAMICS ---\")\n",
        "    print(implicit_threads)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IMPLICIT THREADS AND DYNAMICS ---\n",
            "### Hesitancy or reluctance (even if people “agreed”)\n",
            "- **The “tentative decision” framing is a tell.** Saying *tentative decision: Tom will work over the weekend* suggests people weren’t fully comfortable owning that choice out loud. It reads like an agreement made under pressure rather than enthusiastic consent.\n",
            "- **Implicit reluctance around asking Tom to sacrifice personal time.** Nobody is quoted saying “Tom volunteered” or “Tom offered,” and the rationale is to “recover the schedule” (i.e., protect the plan), which can indicate quiet discomfort: it solves the timeline problem by shifting burden onto one person.\n",
            "- **The concern about compressing testing suggests someone wasn’t buying the optimism.** Even if the team agreed to the weekend work plan, the explicit mention of reduced testing buffer implies at least one person was uneasy about risk and quality—and may have felt they needed to “flag” it to protect themselves later if issues arise.\n",
            "\n",
            "### Underlying disagreements or tensions\n",
            "- **Schedule vs. quality tension is central.** One side is effectively pushing to maintain the launch date; another is worried about final testing being squeezed. The “solution” (weekend work) avoids the hard decision (move the date, de-scope, or reallocate resources), which often means the disagreement is unresolved—just deferred.\n",
            "- **Resourcing imbalance may be a quiet friction point.** Front-end is “ahead of schedule with extra bandwidth,” yet the recovery plan is “Tom works over the weekend,” not “we’ll reassign front-end bandwidth to support backend/testing.” That can imply:\n",
            "  - Silos (front-end can’t easily help backend),\n",
            "  - Or reluctance to renegotiate responsibilities,\n",
            "  - Or a dynamic where certain people are expected to absorb crunch time.\n",
            "- **Marketing vs. engineering risk tolerance is being negotiated.** The marketing push concern (servers + initial bugs) implies someone is worried about reputational damage or operational instability, possibly pushing back against an aggressive launch. The “soft launch” compromise indicates alignment was achieved, but only after acknowledging a real fear: things may break in public.\n",
            "\n",
            "### Overall mood\n",
            "- **Cautiously stressed, pragmatic, and risk-aware.** “Mostly on track” is tempered immediately by complexity and schedule risk. People are problem-solving, but the fixes are pressure-driven (weekend work, buffer management).\n",
            "- **Not openly confrontational, but with an undercurrent of anxiety.** The group is making decisions, yet the language points to managing uncertainty and protecting the launch narrative rather than feeling confident.\n",
            "- **One area of relief/competence:** front-end being ahead of schedule and the marketing plan adjusting to a soft launch suggests the team can adapt—but they’re doing it to contain risk, not because everything is going smoothly.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "Tkb3x_cnSggI",
        "outputId": "eb11d767-b910-4ba7-a02e-44f447dfe6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 7: g5 - Perform \"Intellectual Combinations\"**\n",
        "\n",
        "      * Here, we prompt the AI to be creative and solve a problem by synthesizing different ideas from the meeting."
      ],
      "metadata": {
        "id": "yyd_EZ5HSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: g5 - Generating a Novel Solution\n",
        "prompt_g5 = f\"\"\"\n",
        "Context: In the meeting, Maria suggested a 'soft launch' to avoid server strain, and also mentioned her team has 'extra bandwidth'.\n",
        "Tom is facing a 3-day delay on the backend.\n",
        "\n",
        "Task: Propose a novel, actionable idea that uses Maria's team's extra bandwidth to help mitigate Tom's 3-day delay. Combine these two separate pieces of information into a single solution.\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g5 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g5}]\n",
        "    )\n",
        "    novel_solution = response_g5.choices[0].message.content\n",
        "    print(\"--- NOVEL SOLUTION PROPOSED BY AI ---\")\n",
        "    print(novel_solution)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NOVEL SOLUTION PROPOSED BY AI ---\n",
            "Use Maria’s team’s “extra bandwidth” to run a **soft-launch-focused “backend acceleration squad”** for 72 hours that both reduces immediate load and eliminates Tom’s backend blockers.\n",
            "\n",
            "**Actionable plan (single combined solution):**\n",
            "1. **Stand up a Soft Launch Gate + Throttle Layer (owned by Maria’s team)**\n",
            "   - Implement a feature flag / allowlist (internal users → small pilot cohort) plus request throttling and aggressive caching for the most-used endpoints.\n",
            "   - Result: you can release on schedule to a limited audience without stressing the backend, buying safe time while the full backend work finishes.\n",
            "\n",
            "2. **Parallelize Tom’s delayed backend work by offloading “delay-causing” tasks to Maria’s team**\n",
            "   - Maria’s team takes on the surrounding work that often causes backend delays:\n",
            "     - Writing/validating API contracts (OpenAPI/Swagger) and generating stubs/mocks.\n",
            "     - Building a realistic **mock backend** (or contract test harness) so frontend/integration can proceed immediately.\n",
            "     - Setting up load-test scripts + monitoring dashboards aligned with the soft launch thresholds (so Tom isn’t debugging performance blind later).\n",
            "   - Result: Tom focuses only on the critical backend core; integration and validation proceed in parallel instead of waiting 3 days.\n",
            "\n",
            "3. **Run a 48-hour “Pilot Readiness Sprint”**\n",
            "   - Day 1: Maria’s team ships the soft launch gate + mocks/contracts; frontend/integration starts against mocks.\n",
            "   - Day 2: Maria’s team runs load tests against the throttled/cached paths and tunes limits; Tom merges backend core work as it lands.\n",
            "   - Day 3: Soft launch to pilot cohort with enforced limits; expand gradually as Tom’s remaining backend pieces complete.\n",
            "\n",
            "**Why this mitigates the 3-day delay:**\n",
            "- The **soft launch** reduces risk and load immediately, so you’re not blocked on full backend readiness.\n",
            "- Maria’s team’s **extra bandwidth** converts the delay into parallel progress (contracts, mocks, test harnesses, gating, monitoring), shortening the “critical path” and preventing Tom from becoming the single bottleneck.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "8JN5E5X4SggI",
        "outputId": "a6c7b10d-ef05-44df-bd19-ff436d33d018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 3: Determining the Action (The 'What Next')\n",
        "\n",
        "Finally, we turn the analysis into concrete, forward-looking artifacts.\n",
        "\n",
        "1.  **Cell 8: g6 - Define the Output Format (Final Summary)**\n",
        "\n",
        "      * We compile all the key information into a structured, final summary, showing the importance of clear outputs."
      ],
      "metadata": {
        "id": "J0CD-ztrSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: g6 - Creating the Final, Structured Summary\n",
        "prompt_g6 = f\"\"\"\n",
        "Task: Create a final, concise summary of the meeting in a markdown table.\n",
        "Use the following information to construct the table.\n",
        "\n",
        "- New Developments: {new_developments}\n",
        "\n",
        "The table should have three columns: \"Topic\", \"Decision/Outcome\", and \"Owner\".\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g6 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g6}]\n",
        "    )\n",
        "    final_summary_table = response_g6.choices[0].message.content\n",
        "    print(\"--- FINAL MEETING SUMMARY TABLE ---\")\n",
        "    print(final_summary_table)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FINAL MEETING SUMMARY TABLE ---\n",
            "| Topic | Decision/Outcome | Owner |\n",
            "|---|---|---|\n",
            "| Backend risk (payment gateway integration) | Identified higher-than-expected complexity; potential **~3-day** backend delay (backend otherwise mostly on track). | Backend team / Tom |\n",
            "| Schedule & testing impact | A **3-day slip** would compress final testing and could threaten launch due to limited buffer. | Project team |\n",
            "| Mitigation plan | **Tom will work over the weekend** to try to recover time and avoid delaying the timeline. | Tom |\n",
            "| Front-end status | Front-end is **ahead of schedule**, providing extra bandwidth. | Front-end team |\n",
            "| Launch-day risk | Large launch-day social media push could overload servers if early bugs exist. | Product/Engineering |\n",
            "| Launch strategy | **Soft launch (invite-only) for week 1**, then **major social media push** afterward. | Product/Marketing |\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "xXRnPWuESggI",
        "outputId": "55a40b03-0b45-42ed-e00e-456ed6803a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 9: g7 - Generate the Subsequent Task**\n",
        "\n",
        "      * The last step is to use the analysis to perform a real-world action, closing the loop from insight to action."
      ],
      "metadata": {
        "id": "jk1g9wVvSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: g7 - Drafting the Follow-Up Action\n",
        "prompt_g7 = f\"\"\"\n",
        "Task: Based on the following summary table, draft a polite and professional follow-up email to the team (Sarah, Tom, Maria).\n",
        "The email should clearly state the decisions made and the action items for each person.\n",
        "\n",
        "Summary Table:\n",
        "---\n",
        "{final_summary_table}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g7 = client.chat.completions.create(\n",
        "        model=\"gpt-5.2\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g7}]\n",
        "    )\n",
        "    follow_up_email = response_g7.choices[0].message.content\n",
        "    print(\"--- DRAFT FOLLOW-UP EMAIL ---\")\n",
        "    print(follow_up_email)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DRAFT FOLLOW-UP EMAIL ---\n",
            "Subject: Follow-up: Payment integration risk, mitigation plan, and launch strategy\n",
            "\n",
            "Hi Sarah, Tom, and Maria,  \n",
            "\n",
            "Thanks for today’s discussion. I wanted to summarize the key decisions and confirm the action items coming out of it:\n",
            "\n",
            "**Decisions / Outcomes**\n",
            "- **Backend payment gateway integration risk:** We identified higher-than-expected complexity, with a potential **~3-day backend delay** (everything else is largely on track).\n",
            "- **Schedule & testing impact:** A **3-day slip** would compress final testing and could put launch at risk given the limited buffer.\n",
            "- **Mitigation approach:** We’ll attempt to recover the time rather than move the overall timeline.\n",
            "- **Launch strategy to reduce risk:** We will do a **soft launch (invite-only) for week 1**, followed by a **major social media push** after that to avoid overwhelming servers while early issues are still being validated.\n",
            "\n",
            "**Action Items**\n",
            "- **Tom (Backend)**\n",
            "  - Work over the weekend to push the payment gateway integration forward and **try to recover the potential ~3 days**.\n",
            "  - Share a status update (and revised ETA/risk assessment) as soon as you have a clearer read early next week.\n",
            "\n",
            "- **Sarah (Front-end)**\n",
            "  - Since front-end is ahead of schedule, please identify where you can provide **extra bandwidth** to support risk reduction (e.g., helping with integration validation, UI edge cases, or test support) and coordinate with Tom on what would be most helpful.\n",
            "\n",
            "- **Maria (Product/Marketing)**\n",
            "  - Update the go-to-market plan to reflect the **invite-only soft launch in week 1** and schedule the **major social media push** for after week 1.\n",
            "  - Align messaging/timing with Engineering so we don’t drive peak traffic until we’re confident in stability.\n",
            "\n",
            "If anyone sees gaps or has concerns with the above, please reply-all so we can adjust quickly.  \n",
            "\n",
            "Best regards,  \n",
            "[Your Name]\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "aQ1PzmTlSggI",
        "outputId": "7f72b5d9-57b9-4089-a8e9-1a397ad8dcf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook structure doesn't just \"get a summary.\" It takes the user on a journey, showing them *how* to think with the AI as a partner. It perfectly translates the abstract \"Scope, Investigation, Action\" framework into a repeatable, educational, and powerful process."
      ],
      "metadata": {
        "id": "-tDIxvHSSggJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}